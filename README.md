# STM32_SCREEN_SPI_DCMI_OV7670

Bare metal project on a STM32_F429_DISCO board to extract image from an OV7670 CMOS camera. This is the second project in a sequence of projects.  

## General description
With the base project successfully set up (see STM32_SCREEN_ref_pro project), the next natural step in my engineer mind is to disassemble the project to its core elements and replace everything with custom solutions. Why? Well, that should allow us a lot of flexibility later on when, for instance, we would want to use the LTDC screen driving and the DCMI camera interfacing on our F429 DISCO board at the same time. You see, CubeMX simply doesn’t allow that to happen due to two of the pins overlapping in the F429 DISCO. Manually though, we could just force the solution with minimal data loss (i.e. dropping a DCMI or LTDC pin). Having a custom solution also will allow us to “trim the fat” from the base project further and have something completely bare-bones for the future. Ideal source material for going nuts...by, for instance, slapping a camera to it and building a crude telescope.

## What shall we do then?
Firstly, we don’t want TouchGFX to make us a frame buffer anymore, it will be perfectly adequate for us to have an image generated by us manually. Our task then will be to find out, where the TouchGFX is interfacing with the base code, severe that tie and put something “less complex” on there instead.

Secondly, we can’t “wing” anymore the ILI driving and assume that it moves in mysterious ways. We need to get into it, understand how it works (timing, commands and data transfer) and then remove from the base project's driver what we don’t need. This need will be particularly important since synchronization and proper layering of the screen output will not be done for us through TouchGFX anymore. We will have to set up the data flow (timing and pixel positioning) ourselves. 

Thirdly, once we are rock solid in publishing our constant custom image to the screen, we will take a look into how to drive a simple CMOS camera sensor (here an OV7670). We will be using standard I2C to command the camera but will be receive data from it using a parallel serial interface.

Fourth, we will hook up our CMOS camera to the DISCO board and use the DCMI interface of the STM32F429 to load the image data into the frame buffer.

## Previous relevant projects
The following projects should be checked:
- STM32_SCREEN_ref_pro
- STM32_SPIDriver
- STM32_ClockDriver
- STM32_DMADriver

## To read
Datasheets/reference manuals for the STM32F4xx MCU family, the ILI9341 screen driver and the OV7670 CMOS camera.

## Particularities

### L053 vs F429
Before all these points above, in Zero-th(?) position, we will have to talk a bit about the differences between the MCU we have in our DISCO board (F429) and the NUCLEO board (L053) of previous projects. Generally, it can be said that there is a high level of compatibility between different types of STM32 MCUs regarding code, especially if one relies on the official HAL to do the heavy lifting. When doing bare metal though, this isn’t the case anymore. Registers’ names can be different, for instance. Anyway, porting of existing code would not work at all without minimal (or sometimes major) modifications.

In our case, there are four glaring differences between the L0xx and the F4xx:
1)	Clocking is differently organized. This means that register names are different, bits are different, plus peripherals might be placed on different busses (APB1, APB2, AHB).
2)	The I2C peripheral between the two MCUs is completely different. It is necessary to write a brand-new driver for the F429. More on this below.
3)	We have a significantly more advanced DMA on the F429 with a lot more channels/streams than on the L053. I won’t go into this; it is not complicated to make heads or tails of it using the refman of the F429. The STM32_DMADriver project can be of help.
4)	GPIO MODER register resets to 0x0, not 0xFF.

### Can't TouchGFX this
Investigating the base project, we can see that TouchGFX interfaces with the screen through the ILI9341 driver. More precisely, it calls “ILI9341_SetWindow(x, y, x+w-1, y+h-1)” to set the window we will be writing to on the screen and then the “ILI9341_DrawBitmap(w, h, pixels);” to actually “fill” that window with a bitmap (i.e., with an image). Unfortunately, the width, the height and the pixel pointer (as well as the frame buffer!) for that call are all set up within TouchGFX, so we will have to do that all manually once we have removed TouchGFX.

Checking the “ILI9341.c”, setting the window is just a set of SPI commands, nothing complicated. For drawing the bitmap, we are using a few variables to define the size (same as the window) and a memory pointer with the memory stepping done using the DMA of the SPI. As such, if we manage to generate a frame buffer ourselves, add a pointer to it and define the proper height/width, everything should be fine, the DMA will do the work for us.

As mentioned in the base project description, we have a bit of a memory allocation problem here though since, if we have TouchGFX active, we can’t store an entire image within RAM due to lack of space (we will have at least 70 kB of the 196kB memory used up in RAM with TouchGFX included…while an image is 150 kB). TouchGFX itself gets around this problem by using/generating smaller memory frame buffers and then “reconstruct” the image, section by section, which is a pretty efficient way to save memory and sacrifice speed. I do want to have speed eventually so this method should be omitted. If we don’t use TouchGFX, we "should" be able to store the entire image in RAM…

With no TouchGFX, we can simply define our frame buffer as a variable (an array of 153000 bytes for a 16-bit depth image size of 320x240) and then define a pointer to the first element of the array. The DMA should be able to deal with everything from there then…

…except that it won’t. As it goes, DMA’s need to know, how many elements they should transfer ahead of the transfer and the register to define the number of transfers is capped at 65k (the register is 16-bits wide). As such, we can’t transfer our image in one fell swoop to the screen using the SPI+DMA since each transfer has to be 8 bits to comply the SPI and 65k bytes is less than half the size of our frame buffer/image.

For now, the solution I came up with is to do the image transfer function in sections just as TouchGFX does (see the function “Transmit320x240Frame” in the “image_transfer” source file). I ended up going with a division of 4. Please be aware that in this code each consecutive step must have the frame buffer pointer manually set to the next step with the window set accordingly. Also, since we are using DMA for the transfer and we aren’t blocking, we need to also add a delay between the sections, otherwise we will have data overrun/data loss.

#### What to publish?
With TouchGFX evicted from our code, we don’t have a way to fill up our frame buffer. Thus I wrote an “GenerateImage” function that solves that problem. It is possible to choose between 5 different patterns (see code, just uncomment the one that you fancy).

Of note, using simple patterns, it is extremely likely that one can see the desired outcome on the screen without any investigation of the ILI9341 driver. Unfortunately, when every pixel must go to its exact place (like when we want to publish camera output on a screen), any mishap can end us up with a torn image, a rolling image or nothing at all. I picked the five pattern for the project since my experience suggests that they are all very useful for debugging screen timing problems. If we know what should be on the screen, it is a hell lot easier to understand, where things go tits up.

They also are very similar to the patterns that were used by "FPGA4Fun" in his HDMI project. 

### Screen driving
The ILI9341 screen driver has an in-built memory buffer (GRAM) that supplies the screen with the output. This GRAM is 172800 bytes big, fitting exactly the size of the screen with the porches (blank areas added for positioning). Generally, it is best to load an image into the GRAM and the publish it, albeit it is possible to just bypass the GRAM and funnel all incoming data immediately to the screen. In this latter case, timing can become complicated since all pixels must arrive with the same speed as they should be published in order to have the desired output. Yes, if it hasn't been clear already, screen an image capture timings can become horrible sensitive once people drill too deep into the issue...

Anyway, in our particular case, we have the image/frame buffer "cut" into 4 equal sections. Each section’s size must match the window we set when driving the ILI (with a bunch of SPI commands to place the starting and the end point within the pixel matrix of the screen). The sections must also be retrieved from the RAM in the exact sequence to result in a proper image. Publishing have to also match the orientation of the screen, plus the endian of the data has to be matched. With all that considered, we would need to manipulate the ILI's 36h register to define the frame buffer readout in case it does not match the loading of the buffer (in the code provided, we rotate the frame buffer readout by 270 degrees). The endian of the readout can be set in F6h. An indication of that is if the colours seems to be off.

The ILI runs on its own clock, refreshing the screen from the frame buffer (see B1h register).

We pick the bit depth of the screen in register 3Ah…

…and here I want to get into something. When setting up the TouchGFX example project in "STM32_SCREEN_ref_pro", I mentioned that I don’t understand what we are driving the ILI with since we seem to be hard-wired to SPI, yet have some LTDC activity on the bus. As it turns out, we have TWO different interfaces: one to control the ILI and one to send it data. The control interface is defined by the IM[3:0] bits and they are indeed hard-wired on our board to be SPI interface type II (4 line SPI). We can’t change that. What we can change is how we want to send data to the ILI and once received, what it does with it. It can either directly publish it as it arrives or it can store it within its own GRAM. We can also tell the ILI to capture the incoming data from either a serial RGB interface or from SPI (see F6h, B0h and 3Ah registers). Without telling it anything, it will capture data from SPI and log it into the GRAM, those are the reset values. As such, what happened in the original TouchGFX project was that the LTCD interface was activated in the MCU to send the RGB data to the ILI, resulting in some traffic on those pins. The driving was still using SPI though.

Reading through the datasheet, I came to realise that the ILI driver we have in the base project had a lot more registers manipulated than what we actually needed. I removed most of them, leaving only the most crucial ones. (Of note, we also use registers 11h to turn off sleeping, 29h to turn on the display, 1h to reset the display and 2Ch to tell the ILI we are sending data to it.)

Lastly, I need to talk about the command architecture of the ILI. It is commanded using SPI (thanks to the pre-set IM bits) so we have a CS chip select that will have to be controlled using a normal GPIO output in the F429. We also have the DC pin on the ILI, which is a command input: if it is HIGH, we will be sending it data, if it is LOW, we are sending it a command/register address. The DC pin is also a simple GPIO output.

### Camera driving
The OV7670 is a cheap CMOS camera that was very popular a few years ago. It has a chip size of 640x480 and is driven by I2C with a parallel serial interface to publish data. These days, it is mostly replaced by the OV5640, which is practically the same camera just with higher amount of pixels. I will stick with the OV7670 for simplicity’s sake and because this is the one that I have lying around after the DE0 Digicam project. (If anyone is seriously following me on these projects, swapping out the OV7670 for the OV5640 should not pose any great difficulties, just a bit of annoyance.)

After the initialization commands have been sent, the camera can be put into automatic mode where it will publish images on a continuous basis which is perfect for our application.

Of note, I took the command matrix from the Adafruit OV7670 project frot his camera and used it without any modifications. This I highly recommend as well since, unlike the ILI where “trimming the fat” is possible, experience suggests that mucking around with the registers of the camera (any camera for that matter) can lead to adverse effects very easily. There are also unknown registers that must be set, but nobody seems to know, why exactly (no info of them in the datasheet, but if removed, prepare for a bad time). In order to have a good image quality and an enjoyable output, it is simply necessary to use an available existing init matrix.

There is one more input we must provide to the OV7670: the MCLK or “master clock”. This master clock is used by the camera to run itself and must be of specific frequencies, otherwise the camera will refuse to work. The recommended value is 24 MHz, though here I managed to make it work at 12 MHz as well. I am providing the clocking using our a 50% duty cycle PWM pulse from our MCU (check the ClockDriver project).

Once the I2C and the master clock is set (plus we provided power and ground), the camera will be ready to start publishing its images at 30fps or 60 fps, depending on the resolution setup and the master clock frequency.

We will have 8 data lines as output, plus VSYNCH, HSYNCH and PIXCLK. PIXCLK will be the pixel clock, the frequency at which a pixel (well, half-pixel) leaves the camera. It is extremely important to have a good grip on this signal since this will be the one that times the DCMI serial interface of the MCU. VSYNCH and HSYNCH are vertical and horizontal synch signals, defining the end of a frame and the end of a line, respectively. Their “activity state” can be flipped within the setup of the camera. It is very important to match their profile with the DCMI interface, otherwise no data will be captured (for instance, VSYNCH will be active HIGH for both the camera and the interface in our particular case, some cameras might demand another setup or be flexible).

Lastly, we will have the 8 data lines, which will be publishing 16-bit RGB565 image format, as set by our camera command matrix. That means that one pixel will take two PIXCLK to be sent to over to the MCU with the MSB byte consisting of RRRRRGGG and the LSB byte GGGBBBBB. Yet again, I need to mention that the capture sequence of the pixels must match the publishing sequence of the pixels for a properly coloured output. It is perfectly possible to swap the LSB and the MSB bytes by flipping some of the control bits in the DCMI interface of the camera, leading to some rather interestingly coloured images...

### DCMI
DCMI stands for Digital Camera Interface. It is an in-built serial interface peripheral that allows us to capture incoming data in parallel. That is good news since MCUs are not capable of real parallel activities like FPGAs do, meaning that any incoming data would need to be captured sequentially (just an example, without DCMI, the 24 MHz 8-bit parallel output would need to be captured at a frequency of 8x24 = 192 MHz to avoid data loss).

The DCMI peripheral samples the incoming data on the edge of its own internal clock (AHB2 bus) and being triggered to do so on the edge of the PIXCLK – i.e., the refresh rate of the incoming data flow. It loads a 32-bit data register which, when full, triggers a DMA request (DMA Stream 1, channel 1, see datasheet and refman). As such, the DCMI will not be able to run unless a DMA is set in parallel with it.

Luckily, we won’t be plagued by the DMA limitations here since, unlike with the SPI’s 8-bit width, we will have a width of 32 bits for the DMA. This means that the maximum number of bytes we can transfer without sectioning is 65535 * 4 = 262140 bytes, way above the image size.

All in all, the DCMI is a rather straightforward peripheral with only a few registers to manipulate. The only thing to mention is that we aren’t going into embedded control, we are relying on the camera to drive the interface with its synch signals and pixel clock.

## User guide
The project is VERY noise sensitive: the image easily falls apart, colours going flipped and so on. It is highly recommended to produce a custom adapter PCB to attach the camera directly to the DISCO board, thus decreasing the noise. Having short cables to attach the camera to the DISCO also helps, albeit that can be tricky when one has to manage more than a dozen cables at the same time. (The reason for the noise is that simple jumper cables become unreliable above frequencies of 10 MHz in case their length is greater than 10 cm. Here we will be working with 24 MHz signals eventually, so go figures...)

### Project architecture
Just to give an overview:
1) We are driving/commanding a camera using I2C
2) We clock the camera using PWM
3) We capture the output of the camera (8-bit serial) using the DCMI peripheral
4) We log the captured data into a frame buffer placed within the MCU’s RAM using DMA
5) We command the screen using SPI commands
6) We extract the data from the frame buffer using DMA
7) We send the data to the screen using SPI

All in all, commenting in/out sections in the “main” allows different outputs to emerge on the screen, be that a pattern or the camera’s data.

### I2C on the F429
Unlike in the L053 where we could rely on a lot of automation within the peripheral (such as using AUTOEND to generate a stop bit, or using different registers for Tx/Rx, or publish the slave address in CR2, or flip a bit in the CR1 register to flush our Tx, or have ACK automatically), we will need to do almost everything manually. We need to write functions that generate start/stop bits, one to send the slave address and one to send data.

The start/stop generating functions are rather straight forwards, we merely need to set a bit in the CR1 register and wait for a flag to go HIGH indicating success. Mind, within the start condition, it is also necessary to enable the master’s acknowledge towards the slave.

Addressing is done by writing to the DR register – the only data transfer register – after a start condition and waiting for the ADDR bit to go HIGH. The ADDR bit will ONLY go HIGH in case there is match in the address, so it can only be used for timing if we are sure about the address. Also, there is a redundancy with the AF/acknowledge bit (I am not sure, why this makes sense, but it can hard lock the code pretty easily). For scanning the bus for addresses, the ADDR bit MUST NOT be checked for timing, we need to solely use the AF bit (see code).

We pick Tx or Rx by setting the LSB in the address. This also means that we need to shift left (“<<1”) any address before we put it into the DR register. LSB 0 is Tx, LSB 1 is Rx. We will only have Tx here. 

Timing is done by defining the driving frequency of the peripheral (APB1 peripheral clock for I2C1) within the CR2 register (must be the same value put as the APB1 frequency in MHz), set the division rate in the CCR register and set the rising time in the TRISE register. The calculations to get the CCR and TRISE are within the refman.

Of note, we clean the flags within the peripheral by reading the SR registers. Doing so does clean ALL flags, so tread lightly and not accidentally remove flags that should not be cleaned.

### SPI, DMA, PWM, Clocking
These peripherals are run on code pretty much identical to the L0xx so I won’t be explaining them. Merely a few register names and bits need to be changed from the original code.

I am handling the data flow of the SPI differently though so that I won’t need to change the existing ILI9341 driver. The difference is that we are publishing single bytes on the SPI bus instead of an array as we did before.

## Conclusion
As mentioned, we are cutting a lot of corners in this project and aren’t using any of the more advanced functions such as DMA2D or TouchGFX. We also have a myriad of while loops that block execution unnecessarily.

Nevertheless, the image is captured at a frame rate of around 1 fps.

Next step will be to speed everything up to a more comforting 30 fps (or more) by adjusting the timing, removing while loops and adding LTDC driving to the screen.
